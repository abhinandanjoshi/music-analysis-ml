# music-analysis-ml
This is the M.Tech Final project worked on Music analysis.
ğŸµ Intelligent Music Analysis using Audio Signal Processing & Machine Learning

An end-to-end music intelligence system that analyzes raw audio signals to understand genre, emotion, and similarity using signal processing and machine learning techniques.

ğŸ“Œ Overview

This project focuses on content-based music analysis, leveraging audio signal processing and machine learning to extract meaningful insights directly from music files â€” without relying on user metadata.

The system performs:

ğŸ¼ Music genre classification

ğŸ˜Š Emotion / mood detection

ğŸ” Music similarity & recommendation

ğŸ“Š Rich audio visualizations

The project is designed as an M.Tech-level academic project while following industry best practices for reproducibility, modularity, and experimentation.

ğŸ¯ Motivation

Traditional music recommendation systems heavily depend on user behavior. This project instead explores how machines can â€œunderstandâ€ music itself by analyzing:

Frequency content

Rhythm and tempo

Harmonic and timbral characteristics

Such approaches are widely used in:

Music streaming platforms

Music information retrieval (MIR)

Audio-based recommendation systems

AI-driven media analytics

ğŸ§  Key Concepts Covered

Digital Signal Processing (DSP)

Timeâ€“Frequency Analysis

Feature Engineering on Audio Signals

Classical Machine Learning & Deep Learning

Model Evaluation & Experimentation

End-to-End ML Pipelines

ğŸ§° Tech Stack
ğŸµ Audio & Signal Processing

Librosa

NumPy

SciPy

ğŸ¤– Machine Learning

Scikit-learn

XGBoost

PyTorch (CNNs on spectrograms)

ğŸ“Š Visualization

Matplotlib

Seaborn

ğŸ“ Datasets

GTZAN (Genre Classification)

DEAM (Emotion Analysis in Music)

Additional open-source audio datasets

ğŸ”¬ Feature Extraction

The following features are extracted using Librosa:

Time-Domain Features

Zero Crossing Rate

Root Mean Square (RMS) Energy

Frequency-Domain Features

Spectral Centroid

Spectral Bandwidth

Spectral Roll-off

Spectral Contrast

Cepstral Features

MFCCs (13â€“40 coefficients)

Delta & Delta-Delta MFCCs

Rhythm & Harmony

Tempo (BPM)

Beat Tracking

Chroma Features

Tonnetz Representation

These features form the basis for both machine learning models and music similarity analysis.

ğŸ¤– Machine Learning Models
ğŸ¼ Genre Classification

Random Forest

XGBoost

CNN on Mel-Spectrograms

ğŸ˜Š Emotion / Mood Detection

Regression on Valenceâ€“Arousal space

Multi-class emotion classification (Happy, Sad, Calm, Energetic)

ğŸ” Music Similarity & Recommendation

Feature embeddings

Cosine similarity

k-Nearest Neighbors (k-NN)

ğŸ“ˆ Evaluation Metrics

Accuracy, Precision, Recall, F1-score

Confusion Matrix

RMSE / MAE (for emotion regression)

Cross-validation

ğŸ“Š Visualizations

The project includes rich audio visualizations such as:

Waveform plots

Spectrograms

Mel-Spectrograms

MFCC heatmaps

Feature correlation plots

PCA / t-SNE projections for song embeddings

ğŸ—ï¸ Project Structure
